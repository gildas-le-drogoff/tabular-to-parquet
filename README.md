# ğŸ¦€ Tabular to parquet

## ğŸ“˜ Description

`tabular_to_parquet` est un outil en ligne de commande Ã©crit en **Rust** permettant de convertir des fichiers
tabulaires (CSV, TSV ou formats similaires) en **Apache Parquet**, avec une **dÃ©tection automatique du dÃ©limiteur** et
une **infÃ©rence du schÃ©ma Arrow**.

ConÃ§u pour traiter des fichiers volumineux de maniÃ¨re **dÃ©terministe**, il produit des fichiers
Parquet directement exploitables par les moteurs analytiques modernes tels que DuckDB, Polars, Spark, Pandas ou PyArrow.

Il infÃ¨re le schÃ©ma sans nÃ©cessiter de configuration manuelle.

![DÃ©mo du projet](docs/tabular-to-parquet.demo.gif)

## âš™ï¸ FonctionnalitÃ©s

- **DÃ©tection automatique du dÃ©limiteur** : `,`, `;`, `\t`, `|`, `:`, espace
- **InfÃ©rence automatique des types de colonnes** Ã  partir dâ€™un Ã©chantillon (max. 10 000 lignes)
- **Support complet des types Arrow** :
    - BoolÃ©ens
    - Entiers signÃ©s et non signÃ©s
    - Flottants
    - Dates (`Date32`)
    - Timestamps (`Second`, `Millisecond`, `Microsecond`, `Nanosecond`)
    - Heures (`Time64(Microsecond)`)
    - Texte (`Utf8`, `LargeUtf8`)
    - Binaire (`Binary`, `LargeBinary`)
- **Conversion robuste** :
    - Valeurs invalides converties en `null`
    - Adaptation automatique de la nullabilitÃ© si nÃ©cessaire
- **Traitement par blocs** (50 000 lignes)
- **Ã‰criture Parquet compressÃ©e** avec **ZSTD**
- **Support de lâ€™entrÃ©e standard (`stdin`)** via `-`

## ğŸ§© DÃ©pendances principales

| Crate        | RÃ´le                                                        |
|--------------|-------------------------------------------------------------|
| `anyhow`     | Gestion unifiÃ©e et contextuelle des erreurs                 |
| `csv`        | Lecture et parsing des fichiers tabulaires (CSV, TSV, etc.) |
| `arrow`      | Structures de donnÃ©es colonne et schÃ©mas Apache Arrow       |
| `parquet`    | Ã‰criture du format Apache Parquet                           |
| `chrono`     | Parsing et manipulation des dates, heures et timestamps     |
| `rayon`      | ParallÃ©lisation CPU (infÃ©rence, traitements auxiliaires)    |
| `clap`       | Parsing des arguments de ligne de commande                  |
| `log`        | API de journalisation structurÃ©e                            |
| `indicatif`  | Barres de progression et indicateurs de traitement          |
| `owo-colors` | Colorisation de la sortie terminal                          |

> Les versions de `arrow` et `parquet` doivent Ãªtre identiques.

## ğŸ—ï¸ Installation

### PrÃ©requis

- Rust stable (Ã©dition 2021)
- Cargo

### Compilation

```bash
cargo build --release  # --target x86_64-unknown-linux-musl (pour compatibilitÃ©)
```

Le binaire gÃ©nÃ©rÃ© se trouve dans :

```text
./target/release/tabular_to_parquet
```

## Utilisation

### Syntaxe

```bash
tabular_to_parquet <fichier | ->
```

### Option disponible

```bash
tabular_to_parquet --inferer-schema-complet fichier.(csv|tsv)
```

* `--inferer-schema-complet`
  Analyse lâ€™ensemble du fichier pour lâ€™infÃ©rence du schÃ©ma au lieu dâ€™un Ã©chantillon.
  Cette option augmente le temps dâ€™analyse.

### Exemples

Conversion dâ€™un fichier CSV/TSV :

```bash
tabular_to_parquet donnees.csv
```

Produit le fichier `donnees.parquet` dans le mÃªme rÃ©pertoire.

Conversion dâ€™un fichier TSV dans un sous-rÃ©pertoire :

```bash
tabular_to_parquet ./data/mesures.tsv
```

Produit le fichier `./data/mesures.parquet`.

Lecture depuis lâ€™entrÃ©e standard :

```bash
cat donnees.csv | tabular_to_parquet -
```

Produit le fichier... `stdin.parquet`

## InfÃ©rence du schÃ©ma

- Analyse dâ€™un Ã©chantillon des **10 000 premiÃ¨res lignes**
- Reconnaissance des boolÃ©ens, entiers, flottants
- DÃ©tection des dates et timestamps (formats multiples, UNIX)
- Bascule vers texte en cas dâ€™ambiguÃ¯tÃ©

Toutes les colonnes sont traitÃ©es comme **nullables**.

## ğŸ’¾ Performances

DÃ©bit typique : ~10â´ lignes/s (â‰ˆ 10â€“50 Âµs/ligne).

Le traitement est effectuÃ© par blocs de **50 000 lignes**, avec une Ã©criture sÃ©quentielle dans le fichier Parquet.

Les performances dÃ©pendent fortement :

- du disque (SSD vs HDD)
- de la complexitÃ© du schÃ©ma
- du taux dâ€™erreurs de parsing

Le programme est conÃ§u pour Ãªtre **stable et prÃ©visible**.

## ğŸ§ª VÃ©rification du fichier Parquet

### DuckDB

```sql
SELECT *
  FROM read_parquet('jeu_test_types_complet.parquet') LIMIT 5;
DESCRIBE SELECT * FROM 'jeu_test_types_complet.parquet';
```

### Python (PyArrow / Pandas)

```python
import pyarrow.parquet as pq
table = pq.read_table("jeu_test_types_complet.parquet")
print(table.schema)
print(table.to_pandas().head())
```

### Polars

```python
import polars as pl
df = pl.read_parquet("jeu_test_types_complet.parquet")
print(df.head())
```

## âš ï¸ Limitations

- Pas de paramÃ¨tres CLI avancÃ©s (`--output`, `--delimiter`, etc.)
- Pas de streaming pur
- Encodage supposÃ© UTF-8
- Formats datetime exotiques non reconnus â†’ texte


